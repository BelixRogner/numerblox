{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_formatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"# default_exp model_pipeline\";\n                var nbb_formatted_code = \"# default_exp model_pipeline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality below uses the `Dataset`, `PreProcessor`, `Model` and `PostProcessor` objects to easily propagate\n",
    "data through the processing+model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"#export\\nfrom typing import List\\nfrom tqdm.auto import tqdm\\nfrom typeguard import typechecked\\nfrom rich import print as rich_print\\n\\nfrom numerai_blocks.dataset import Dataset\\nfrom numerai_blocks.preprocessing import BaseProcessor, CopyPreProcessor, display_processor_info\\nfrom numerai_blocks.model import BaseModel\";\n                var nbb_formatted_code = \"# export\\nfrom typing import List\\nfrom tqdm.auto import tqdm\\nfrom typeguard import typechecked\\nfrom rich import print as rich_print\\n\\nfrom numerai_blocks.dataset import Dataset\\nfrom numerai_blocks.preprocessing import (\\n    BaseProcessor,\\n    CopyPreProcessor,\\n    display_processor_info,\\n)\\nfrom numerai_blocks.model import BaseModel\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "import uuid\n",
    "from typing import List\n",
    "from tqdm.auto import tqdm\n",
    "from typeguard import typechecked\n",
    "from rich import print as rich_print\n",
    "\n",
    "from numerai_blocks.dataset import Dataset\n",
    "from numerai_blocks.preprocessing import BaseProcessor, CopyPreProcessor, display_processor_info\n",
    "from numerai_blocks.model import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ModelPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelPipeline` handles all preprocessing, model prediction and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass ModelPipeline:\\n    def __init__(self,\\n                 pipeline_name: str,\\n                 model: BaseModel,\\n                 preprocessors: List[BaseProcessor] = None,\\n                 postprocessors: List[BaseProcessor] = None,\\n                 copy_first = True):\\n        self.pipeline_name = pipeline_name\\n        self.model = model\\n        self.preprocessors = preprocessors\\n        if copy_first:\\n            self.preprocessors.insert(0, CopyPreProcessor())\\n        self.postprocessors = postprocessors\\n\\n    def preprocess(self, dataset: Dataset) -> Dataset:\\n        for preprocessor in tqdm(self.preprocessors,\\n                                 desc=f\\\"{self.pipeline_name} Preprocessing:\\\",\\n                                 position=0):\\n            rich_print(f\\\":car: Applying preprocessing {preprocessor.__class__.__name__} :car:\\\")\\n            dataset = preprocessor(dataset)\\n        return dataset\\n\\n    def postprocess(self, dataset: Dataset) -> Dataset:\\n        for postprocessor in tqdm(self.postprocessors,\\n                                  desc=f\\\"{self.pipeline_name} Postprocessing: \\\",\\n                                  position=0):\\n            rich_print(f\\\":car: Applying postprocessing {postprocessor.__class__.__name} :car:\\\")\\n            dataset = postprocessor(dataset)\\n        return dataset\\n\\n    @display_processor_info\\n    def pipeline(self, dataset: Dataset):\\n        preprocessed_dataset = self.preprocess(dataset)\\n        prediction_dataset = self.model(preprocessed_dataset)\\n        processed_prediction_dataset = self.postprocess(prediction_dataset)\\n        return processed_prediction_dataset\\n\\n    def __call__(self, dataset: Dataset):\\n        return self.pipeline(dataset)\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass ModelPipeline:\\n    def __init__(\\n        self,\\n        pipeline_name: str,\\n        model: BaseModel,\\n        preprocessors: List[BaseProcessor] = None,\\n        postprocessors: List[BaseProcessor] = None,\\n        copy_first=True,\\n    ):\\n        self.pipeline_name = pipeline_name\\n        self.model = model\\n        self.preprocessors = preprocessors\\n        if copy_first:\\n            self.preprocessors.insert(0, CopyPreProcessor())\\n        self.postprocessors = postprocessors\\n\\n    def preprocess(self, dataset: Dataset) -> Dataset:\\n        for preprocessor in tqdm(\\n            self.preprocessors, desc=f\\\"{self.pipeline_name} Preprocessing:\\\", position=0\\n        ):\\n            rich_print(\\n                f\\\":car: Applying preprocessing {preprocessor.__class__.__name__} :car:\\\"\\n            )\\n            dataset = preprocessor(dataset)\\n        return dataset\\n\\n    def postprocess(self, dataset: Dataset) -> Dataset:\\n        for postprocessor in tqdm(\\n            self.postprocessors,\\n            desc=f\\\"{self.pipeline_name} Postprocessing: \\\",\\n            position=0,\\n        ):\\n            rich_print(\\n                f\\\":car: Applying postprocessing {postprocessor.__class__.__name} :car:\\\"\\n            )\\n            dataset = postprocessor(dataset)\\n        return dataset\\n\\n    @display_processor_info\\n    def pipeline(self, dataset: Dataset):\\n        preprocessed_dataset = self.preprocess(dataset)\\n        prediction_dataset = self.model(preprocessed_dataset)\\n        processed_prediction_dataset = self.postprocess(prediction_dataset)\\n        return processed_prediction_dataset\\n\\n    def __call__(self, dataset: Dataset):\\n        return self.pipeline(dataset)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class ModelPipeline:\n",
    "    \"\"\"\n",
    "    Execute all preprocessing, prediction and postprocessing for a given setup.\n",
    "\n",
    "    :param model: A numerai-blocks Model that add prediction columns to a given input Dataset\n",
    "    :param preprocessors: List of initialized (!) PreProcessors.\n",
    "    :param postprocessors: List of initialized (!) PostProcessors.\n",
    "    :param copy_first: Whether to copy the Dataset as a first preprocessing step.\n",
    "    Highly recommended in order to avoid accidentally manipulating the original Dataset and/or DataFrame.\n",
    "    :param pipeline_name: Name for display purposes\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: BaseModel,\n",
    "                 preprocessors: List[BaseProcessor] = None,\n",
    "                 postprocessors: List[BaseProcessor] = None,\n",
    "                 copy_first = True,\n",
    "                 pipeline_name: str = None):\n",
    "        self.pipeline_name = pipeline_name if pipeline_name else uuid.uuid4().hex\n",
    "        self.model = model\n",
    "        self.preprocessors = preprocessors\n",
    "        if copy_first:\n",
    "            self.preprocessors.insert(0, CopyPreProcessor())\n",
    "        self.postprocessors = postprocessors\n",
    "\n",
    "    def preprocess(self, dataset: Dataset) -> Dataset:\n",
    "        for preprocessor in tqdm(self.preprocessors,\n",
    "                                 desc=f\"{self.pipeline_name} Preprocessing:\",\n",
    "                                 position=0):\n",
    "            rich_print(f\":car: Applying preprocessing {preprocessor.__class__.__name__} :car:\")\n",
    "            dataset = preprocessor(dataset)\n",
    "        return dataset\n",
    "\n",
    "    def postprocess(self, dataset: Dataset) -> Dataset:\n",
    "        for postprocessor in tqdm(self.postprocessors,\n",
    "                                  desc=f\"{self.pipeline_name} Postprocessing: \",\n",
    "                                  position=0):\n",
    "            rich_print(f\":car: Applying postprocessing {postprocessor.__class__.__name} :car:\")\n",
    "            dataset = postprocessor(dataset)\n",
    "        return dataset\n",
    "\n",
    "    @display_processor_info\n",
    "    def pipeline(self, dataset: Dataset) -> Dataset:\n",
    "        preprocessed_dataset = self.preprocess(dataset)\n",
    "        prediction_dataset = self.model(preprocessed_dataset)\n",
    "        processed_prediction_dataset = self.postprocess(prediction_dataset)\n",
    "        return processed_prediction_dataset\n",
    "\n",
    "    def __call__(self, dataset: Dataset):\n",
    "        return self.pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ModelPipelineCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelPipelineCollection` wraps multiple `ModelPipelines` to easily run them in sequence.\n",
    "\n",
    "TODO: Add multiprocessing support?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"#export\\n@typechecked\\nclass ModelPipelineCollection:\\n    def __init__(self, pipelines: List[ModelPipeline]):\\n        self.pipelines = {pipe.pipeline_name: pipe for pipe in pipelines}\\n        self.pipeline_names = list(self.pipelines.keys())\\n\\n    def process_all_pipelines(self, dataset: Dataset):\\n        for name, pipeline in tqdm(self.pipelines.items(),\\n                                   desc=\\\"Processing Pipeline Collection\\\"):\\n            self.process_single_pipeline(dataset, name)\\n\\n    def process_single_pipeline(self, dataset: Dataset, pipeline_name: str):\\n        rich_print(f\\\":construction_worker: [bold green]Processing model pipeline:[/bold green] '{pipeline_name}' :construction_worker:\\\")\\n        pipeline = self.get_pipeline(pipeline_name)\\n        pipeline(dataset)\\n\\n    def get_pipeline(self, pipeline_name: str) -> ModelPipeline:\\n        available_pipelines = self.pipeline_names\\n        assert pipeline_name in available_pipelines, f\\\"Requested pipeline '{pipeline_name}', but only the following models are in the collection: '{available_pipelines}'.\\\"\\n        return self.pipelines[pipeline_name]\\n\\n    def __call__(self, dataset: Dataset):\\n        self.process_all_pipelines(dataset=dataset)\";\n                var nbb_formatted_code = \"# export\\n@typechecked\\nclass ModelPipelineCollection:\\n    def __init__(self, pipelines: List[ModelPipeline]):\\n        self.pipelines = {pipe.pipeline_name: pipe for pipe in pipelines}\\n        self.pipeline_names = list(self.pipelines.keys())\\n\\n    def process_all_pipelines(self, dataset: Dataset):\\n        for name, pipeline in tqdm(\\n            self.pipelines.items(), desc=\\\"Processing Pipeline Collection\\\"\\n        ):\\n            self.process_single_pipeline(dataset, name)\\n\\n    def process_single_pipeline(self, dataset: Dataset, pipeline_name: str):\\n        rich_print(\\n            f\\\":construction_worker: [bold green]Processing model pipeline:[/bold green] '{pipeline_name}' :construction_worker:\\\"\\n        )\\n        pipeline = self.get_pipeline(pipeline_name)\\n        pipeline(dataset)\\n\\n    def get_pipeline(self, pipeline_name: str) -> ModelPipeline:\\n        available_pipelines = self.pipeline_names\\n        assert (\\n            pipeline_name in available_pipelines\\n        ), f\\\"Requested pipeline '{pipeline_name}', but only the following models are in the collection: '{available_pipelines}'.\\\"\\n        return self.pipelines[pipeline_name]\\n\\n    def __call__(self, dataset: Dataset):\\n        self.process_all_pipelines(dataset=dataset)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "@typechecked\n",
    "class ModelPipelineCollection:\n",
    "    \"\"\"\n",
    "    Execute multiple initialized ModelPipelines in a sequence.\n",
    "    :param pipelines: List of initialized ModelPipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self, pipelines: List[ModelPipeline]):\n",
    "        self.pipelines = {pipe.pipeline_name: pipe for pipe in pipelines}\n",
    "        self.pipeline_names = list(self.pipelines.keys())\n",
    "\n",
    "    def process_all_pipelines(self, dataset: Dataset) -> Dataset:\n",
    "        for name, pipeline in tqdm(self.pipelines.items(),\n",
    "                                   desc=\"Processing Pipeline Collection\"):\n",
    "            dataset = self.process_single_pipeline(dataset, name)\n",
    "        return dataset\n",
    "\n",
    "    def process_single_pipeline(self, dataset: Dataset, pipeline_name: str) -> Dataset:\n",
    "        rich_print(f\":construction_worker: [bold green]Processing model pipeline:[/bold green] '{pipeline_name}' :construction_worker:\")\n",
    "        pipeline = self.get_pipeline(pipeline_name)\n",
    "        dataset = pipeline(dataset)\n",
    "        return dataset\n",
    "\n",
    "    def get_pipeline(self, pipeline_name: str) -> ModelPipeline:\n",
    "        available_pipelines = self.pipeline_names\n",
    "        assert pipeline_name in available_pipelines, f\"Requested pipeline '{pipeline_name}', but only the following models are in the collection: '{available_pipelines}'.\"\n",
    "        return self.pipelines[pipeline_name]\n",
    "\n",
    "    def __call__(self, dataset: Dataset) -> Dataset:\n",
    "        return self.process_all_pipelines(dataset=dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
