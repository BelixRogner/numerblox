{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_formatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%load_ext lab_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"# default_exp evaluation\";\n                var nbb_formatted_code = \"# default_exp evaluation\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluators take `ModelPipeline` objects as input and can run several validation regimes on them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"from abc import ABC, abstractmethod\\n\\nfrom numerai_blocks.dataset import Dataset, create_dataset\\nfrom numerai_blocks.model_pipeline import ModelPipeline\";\n                var nbb_formatted_code = \"from abc import ABC, abstractmethod\\n\\nfrom numerai_blocks.dataset import Dataset, create_dataset\\nfrom numerai_blocks.model_pipeline import ModelPipeline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from numerai_blocks.dataset import Dataset, create_dataset\n",
    "from numerai_blocks.postprocessing import FeatureNeutralizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"class BaseEvaluator(ABC):\\n    def __init__(self):\\n        ...\\n\\n    @abstractmethod\\n    def evaluate(self, dataset: Dataset, pipeline: ModelPipeline, *args, **kwargs):\\n        ...\";\n                var nbb_formatted_code = \"class BaseEvaluator(ABC):\\n    def __init__(self):\\n        ...\\n\\n    @abstractmethod\\n    def evaluate(self, dataset: Dataset, pipeline: ModelPipeline, *args, **kwargs):\\n        ...\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class BaseEvaluator:\n",
    "    def __init__(self, era_col: str = \"era\", fast_mode = False):\n",
    "        self.era_col = era_col\n",
    "        self.fast_mode = fast_mode\n",
    "\n",
    "    def full_evaluation(self,\n",
    "                        dataset: Dataset,\n",
    "                        example_col: str,\n",
    "                        target_col: str = \"target\"\n",
    "                        ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Perform evaluation for each prediction column in the Dataset\n",
    "        against give target and example prediction column.\n",
    "        \"\"\"\n",
    "        val_stats = pd.DataFrame()\n",
    "        for pred_col in tqdm(dataset.prediction_cols, desc=\"Evaluation: \"):\n",
    "            col_stats = self.evaluation_one_col(dataset=dataset,pred_col=pred_col,\n",
    "                                                target_col=target_col,\n",
    "                                                example_col=example_col)\n",
    "            val_stats = pd.concat([val_stats, col_stats], axis=0)\n",
    "        return val_stats\n",
    "\n",
    "    def evaluation_one_col(self, dataset: Dataset, pred_col: str, target_col: str, example_col: str):\n",
    "        \"\"\"\n",
    "        Perform evaluation for one prediction column\n",
    "        against given target and example prediction column.\n",
    "        \"\"\"\n",
    "        col_stats = pd.DataFrame()\n",
    "        # Compute stats\n",
    "        val_corrs = self.per_era_corrs(dataf=dataset.dataf,\n",
    "                                        pred_col=pred_col,\n",
    "                                        target_col=target_col\n",
    "                                       )\n",
    "        mean, std, sharpe = self.mean_std_sharpe(era_corrs=val_corrs)\n",
    "        max_drawdown = self.max_drawdown(era_corrs=val_corrs)\n",
    "        apy = self.apy(era_corrs=val_corrs)\n",
    "        example_corr = self.example_correlation(dataset=dataset,\n",
    "                                                pred_col=pred_col,\n",
    "                                                example_col=example_col\n",
    "                                                )\n",
    "        mmc_mean, mmc_std, mmc_sharpe = self.mmc(dataf=dataset.dataf,\n",
    "                                                 pred_col=pred_col,\n",
    "                                                 target_col=target_col,\n",
    "                                                 example_col=example_col\n",
    "                                                 )\n",
    "\n",
    "        col_stats[pred_col, \"target\"] = target_col\n",
    "        col_stats.loc[pred_col, \"mean\"] = mean\n",
    "        col_stats.loc[pred_col, \"std\"] = std\n",
    "        col_stats.loc[pred_col, \"sharpe\"] = sharpe\n",
    "        col_stats.loc[pred_col, \"max_drawdown\"] = max_drawdown\n",
    "        col_stats.loc[pred_col, \"apy\"] = apy\n",
    "        col_stats.loc[pred_col, \"mmc_mean\"] = mmc_mean\n",
    "        col_stats.loc[pred_col, \"mmc_std\"] = mmc_std\n",
    "        col_stats.loc[pred_col, \"mmc_sharpe\"] = mmc_sharpe\n",
    "        col_stats.loc[pred_col, \"corr_with_example_preds\"] = example_corr\n",
    "\n",
    "        # Compute intensive stats\n",
    "        if not self.fast_mode:\n",
    "            max_feature_exposure = self.max_feature_exposure(dataset=dataset, pred_col=pred_col)\n",
    "            fn_mean = self.feature_neutral_mean(dataset=dataset, pred_col=pred_col)\n",
    "            tb200_mean, tb200_std, tb200_sharpe = self.tbx_mean_std_sharpe(dataf=dataset.dataf,\n",
    "                                                                           pred_col=pred_col,\n",
    "                                                                           target_col=target_col,\n",
    "                                                                           tb=200)\n",
    "            col_stats.loc[pred_col, \"max_feature_exposure\"] = max_feature_exposure\n",
    "            col_stats.loc[pred_col, \"feature_neutral_mean\"] = fn_mean\n",
    "            col_stats.loc[pred_col, \"tb200_mean\"] = tb200_mean\n",
    "            col_stats.loc[pred_col, \"tb200_std\"] = tb200_std\n",
    "            col_stats.loc[pred_col, \"tb200_sharpe\"] = tb200_sharpe\n",
    "        return col_stats\n",
    "\n",
    "    def per_era_corrs(self, dataf: pd.DataFrame, pred_col: str,\n",
    "                      target_col: str) -> pd.Series:\n",
    "        \"\"\" Correlation between prediction and target for each era. \"\"\"\n",
    "        return dataf.groupby(dataf[self.era_col])\\\n",
    "            .apply(lambda d: self._normalize_uniform(d[pred_col])\n",
    "                   .corr(self._normalize_uniform(d[target_col])))\n",
    "\n",
    "    def mean_std_sharpe(self, era_corrs: pd.Series) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        mean = era_corrs.mean()\n",
    "        std = era_corrs.std(ddof=0)\n",
    "        sharpe = mean / std\n",
    "        return mean, std, sharpe\n",
    "\n",
    "    @staticmethod\n",
    "    def max_drawdown(era_corrs: pd.Series) -> np.float64:\n",
    "        # arbitrarily large window\n",
    "        rolling_max = (era_corrs + 1).cumprod().rolling(window=9000,\n",
    "                                                        min_periods=1).max()\n",
    "        daily_value = (era_corrs + 1).cumprod()\n",
    "        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n",
    "        return max_drawdown\n",
    "\n",
    "    @staticmethod\n",
    "    def apy(era_corrs: pd.Series) -> np.float64:\n",
    "        payout_scores = era_corrs.clip(-0.25, 0.25)\n",
    "        payout_daily_value = (payout_scores + 1).cumprod()\n",
    "        apy = (\n",
    "                      (\n",
    "                              (payout_daily_value.dropna().iloc[-1])\n",
    "                              ** (1 / len(payout_scores))\n",
    "                      )\n",
    "                      ** 49  # 52 weeks of compounding minus 3 for stake compounding lag\n",
    "                      - 1\n",
    "              ) * 100\n",
    "        return apy\n",
    "\n",
    "    def example_correlation(self, dataset: Dataset,\n",
    "                            pred_col: str, example_col: str):\n",
    "        \"\"\" Get correlations with example predictions. \"\"\"\n",
    "        return self.per_era_corrs(dataf=dataset.dataf,\n",
    "                                  pred_col=pred_col,\n",
    "                                  target_col=example_col,\n",
    "                                  ).mean()\n",
    "\n",
    "    def max_feature_exposure(self, dataset: Dataset, pred_col: str) -> np.float64:\n",
    "        max_per_era = dataset.dataf.groupby(self.era_col).apply(\n",
    "            lambda d: d[dataset.feature_cols].corrwith(d[pred_col]).abs().max())\n",
    "        max_feature_exposure = max_per_era.mean()\n",
    "        return max_feature_exposure\n",
    "\n",
    "    @staticmethod\n",
    "    def feature_neutral_mean(dataset: Dataset, pred_col: str) -> np.float64:\n",
    "        fn = FeatureNeutralizer(pred_name=pred_col,\n",
    "                                proportion=1.0)\n",
    "        neutralized_dataset = fn.transform(dataset=dataset)\n",
    "        return neutralized_dataset[fn.final_col_name].mean()\n",
    "\n",
    "    def tbx_mean_std_sharpe(self, dataf: pd.DataFrame,\n",
    "                             pred_col: str,\n",
    "                             target_col: str,\n",
    "                             tb = 200\n",
    "                             ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        tb_val_corrs = self._score_by_date(dataf, [pred_col], target_col, tb=tb)\n",
    "        return self.mean_std_sharpe(era_corrs=tb_val_corrs)\n",
    "\n",
    "    def mmc(self, dataf: pd.DataFrame,\n",
    "            pred_col: str,\n",
    "            target_col: str,\n",
    "            example_col: str\n",
    "            ) -> Tuple[np.float64, np.float64, np.float64]:\n",
    "        \"\"\" MMC over validation. \"\"\"\n",
    "        mmc_scores = []\n",
    "        corr_scores = []\n",
    "        fn = FeatureNeutralizer()\n",
    "        for _, x in dataf.groupby(self.era_col):\n",
    "            series = fn.neutralize(self._normalize_uniform(x[pred_col]), (x[example_col]))\n",
    "            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29 ** 2))\n",
    "            corr_scores.append(self._normalize_uniform(x[pred_col]).corr(x[target_col]))\n",
    "\n",
    "        val_mmc_mean = np.mean(mmc_scores)\n",
    "        val_mmc_std = np.std(mmc_scores)\n",
    "        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
    "        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
    "        return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe\n",
    "\n",
    "    def _score_by_date(self, df: pd.DataFrame, columns: list, target: str, tb=None):\n",
    "        unique_eras = df[self.era_col].unique()\n",
    "        computed = []\n",
    "        for u in unique_eras:\n",
    "            df_era = df[df[self.era_col] == u]\n",
    "            era_pred = np.float64(df_era[columns].values.T)\n",
    "            era_target = np.float64(df_era[target].values.T)\n",
    "\n",
    "            if tb is None:\n",
    "                ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n",
    "            else:\n",
    "                tbidx = np.argsort(era_pred, axis=1)\n",
    "                tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n",
    "                ccs = [np.corrcoef(era_target[idx], pred[idx])[0, 1] for idx, pred in zip(tbidx, era_pred)]\n",
    "                ccs = np.array(ccs)\n",
    "            computed.append(ccs)\n",
    "        return pd.DataFrame(np.array(computed), columns=columns, index=df[self.era_col].unique())\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_uniform(df: pd.DataFrame) -> pd.Series:\n",
    "        x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
    "        return pd.Series(x, index=df.index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Numerai Classic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"class BaseEvaluator(ABC):\\n    def __init__(self):\\n        ...\\n\\n    @abstractmethod\\n    def evaluate(self, dataset: Dataset, pipeline: ModelPipeline, *args, **kwargs):\\n        ...\";\n                var nbb_formatted_code = \"class BaseEvaluator(ABC):\\n    def __init__(self):\\n        ...\\n\\n    @abstractmethod\\n    def evaluate(self, dataset: Dataset, pipeline: ModelPipeline, *args, **kwargs):\\n        ...\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "class NumeraiClassicEvaluator(BaseEvaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Numerai Signals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"class BaseEvaluator(ABC):\\n    def __init__(self):\\n        ...\\n\\n    @abstractmethod\\n    def evaluate(self, dataset: Dataset, pipeline: ModelPipeline, *args, **kwargs):\\n        ...\";\n                var nbb_formatted_code = \"class BaseEvaluator(ABC):\\n    def __init__(self):\\n        ...\\n\\n    @abstractmethod\\n    def evaluate(self, dataset: Dataset, pipeline: ModelPipeline, *args, **kwargs):\\n        ...\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "class NumeraiSignalsEvaluator(BaseEvaluator):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_download.ipynb.\n",
      "Converted 02_dataset.ipynb.\n",
      "Converted 03_preprocessing.ipynb.\n",
      "Converted 04_model.ipynb.\n",
      "Converted 05_postprocessing.ipynb.\n",
      "Converted 06_modelpipeline.ipynb.\n",
      "Converted 07_evaluation.ipynb.\n",
      "Converted 08_key.ipynb.\n",
      "Converted 09_submission.ipynb.\n",
      "Converted 10_staking.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_formatted_code = \"# hide\\n# Run this cell to sync all changes with library\\nfrom nbdev.export import notebook2script\\n\\nnotebook2script()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# Run this cell to sync all changes with library\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}